{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ТЗ\n",
    "# Урок 2. Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mnist\n",
    "#!pip install keras\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простая нейросеть на Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3859 - accuracy: 0.8855\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2044 - accuracy: 0.9395\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1576 - accuracy: 0.9532\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1343 - accuracy: 0.9589\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1167 - accuracy: 0.9641\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9624\n",
      "[7 2 1 0 4]\n",
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# The full neural network code!\n",
    "###############################\n",
    "import numpy as np\n",
    "import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Flatten the images.\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=5,\n",
    "  batch_size=32,\n",
    ")\n",
    "\n",
    "# Evaluate the model.\n",
    "model.evaluate(\n",
    "  test_images,\n",
    "  to_categorical(test_labels)\n",
    ")\n",
    "\n",
    "# Save the model to disk.\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4479 - accuracy: 0.8620\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2046 - accuracy: 0.9378\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1613 - accuracy: 0.9510\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1383 - accuracy: 0.9583\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1234 - accuracy: 0.9629\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1107 - accuracy: 0.9664\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1047 - accuracy: 0.9682\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0940 - accuracy: 0.9713\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0897 - accuracy: 0.9728\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0823 - accuracy: 0.9746\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1334 - accuracy: 0.9628\n",
      "[7 2 1 0 4]\n",
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# Наш вариант модели, с изменными параметрами \n",
    "###############################\n",
    "import numpy as np\n",
    "import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Flatten the images.\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(48, activation='relu'),\n",
    "  Dense(32, activation='relu'),\n",
    "  Dense(24, activation='relu'),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=10,\n",
    "  batch_size=32,\n",
    ")\n",
    "\n",
    "# Evaluate the model.\n",
    "model.evaluate(\n",
    "  test_images,\n",
    "  to_categorical(test_labels)\n",
    ")\n",
    "\n",
    "# Save the model to disk.\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод 1\n",
    "\n",
    "1. Увеличение кол-во слоев в нейросети дает прирост точности и соответственно снижение ошибки только в случае увеличения количества эпох. Что подтверждает предыдущие логичные мысли о том, что сети \"нужно время\" чтобы до обучится. \n",
    "\n",
    "2. При этом отмечаю, что это не ведет к значительному росту точности на тестовой выборке. Рост в сотые доли процента мне кажется не стоит увеличения затрат мощностей на расчеты. Хотя пока они и не критичны. С другой стороны, многое может завесить от задачи. 0.04% от миллиона это будет 400 и если это чьи-то жизни, то расклад может быть другим.\n",
    "\n",
    "Потому, конечно, всегда надо соизмерять затраченные ресурсы с полученным результатом или ожидаемым результатом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4529 - accuracy: 0.8610\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1854 - accuracy: 0.9492\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1498 - accuracy: 0.9573\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1225 - accuracy: 0.9653\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1098 - accuracy: 0.9692\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0941 - accuracy: 0.9733\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0869 - accuracy: 0.9758\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0784 - accuracy: 0.9777\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0734 - accuracy: 0.9796\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0666 - accuracy: 0.9814\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0633 - accuracy: 0.9823\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0586 - accuracy: 0.9834\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0539 - accuracy: 0.9846\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0522 - accuracy: 0.9853\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0476 - accuracy: 0.9867\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0471 - accuracy: 0.9870\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0442 - accuracy: 0.9872\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0444 - accuracy: 0.9878\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0403 - accuracy: 0.9888\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0392 - accuracy: 0.9891\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0390 - accuracy: 0.9895\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0353 - accuracy: 0.9901\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0366 - accuracy: 0.9900\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0342 - accuracy: 0.9909\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0350 - accuracy: 0.9907\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0343 - accuracy: 0.9905\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0325 - accuracy: 0.9917\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0315 - accuracy: 0.9919\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0315 - accuracy: 0.9917\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0287 - accuracy: 0.9925\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0292 - accuracy: 0.9924\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0300 - accuracy: 0.9920\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0284 - accuracy: 0.9917\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0286 - accuracy: 0.9927\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0269 - accuracy: 0.9931\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0235 - accuracy: 0.9940\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0276 - accuracy: 0.9928\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0243 - accuracy: 0.9937\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0292 - accuracy: 0.9929\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0248 - accuracy: 0.9931\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0232 - accuracy: 0.9940\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0246 - accuracy: 0.9940\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0223 - accuracy: 0.9945\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0244 - accuracy: 0.9939\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0273 - accuracy: 0.9934\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0217 - accuracy: 0.9947\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0228 - accuracy: 0.9945\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0220 - accuracy: 0.9947\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0259 - accuracy: 0.9936\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0341 - accuracy: 0.9927\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0231 - accuracy: 0.9945\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0178 - accuracy: 0.9952\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0223 - accuracy: 0.9948\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0206 - accuracy: 0.9946\n",
      "Epoch 55/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0232 - accuracy: 0.9942\n",
      "Epoch 56/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0219 - accuracy: 0.9950\n",
      "Epoch 57/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0253 - accuracy: 0.9944\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0224 - accuracy: 0.9948\n",
      "Epoch 59/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0212 - accuracy: 0.9950\n",
      "Epoch 60/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0214 - accuracy: 0.9945\n",
      "Epoch 61/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0195 - accuracy: 0.9954\n",
      "Epoch 62/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0250 - accuracy: 0.9946\n",
      "Epoch 63/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0214 - accuracy: 0.9945\n",
      "Epoch 64/100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0211 - accuracy: 0.9953\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0339 - accuracy: 0.9924\n",
      "Epoch 66/100\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0269 - accuracy: 0.9943\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0201 - accuracy: 0.9950\n",
      "Epoch 68/100\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0230 - accuracy: 0.9945\n",
      "Epoch 69/100\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0178 - accuracy: 0.9962\n",
      "Epoch 70/100\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0232 - accuracy: 0.9949\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0212 - accuracy: 0.9946\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0255 - accuracy: 0.9941\n",
      "Epoch 73/100\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0193 - accuracy: 0.9955\n",
      "Epoch 74/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0204 - accuracy: 0.9952\n",
      "Epoch 75/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0238 - accuracy: 0.9945\n",
      "Epoch 76/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0239 - accuracy: 0.9945\n",
      "Epoch 77/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0291 - accuracy: 0.9935\n",
      "Epoch 78/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0173 - accuracy: 0.9965\n",
      "Epoch 79/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0365 - accuracy: 0.9915\n",
      "Epoch 80/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0183 - accuracy: 0.9957\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0347 - accuracy: 0.9921\n",
      "Epoch 82/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0281 - accuracy: 0.9937\n",
      "Epoch 83/100\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0227 - accuracy: 0.9945\n",
      "Epoch 84/100\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0188 - accuracy: 0.9955\n",
      "Epoch 85/100\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0227 - accuracy: 0.9951\n",
      "Epoch 86/100\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0263 - accuracy: 0.9943\n",
      "Epoch 87/100\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0229 - accuracy: 0.9944\n",
      "Epoch 88/100\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0183 - accuracy: 0.9957\n",
      "Epoch 89/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0364 - accuracy: 0.9923\n",
      "Epoch 90/100\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0203 - accuracy: 0.9954\n",
      "Epoch 91/100\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0206 - accuracy: 0.9952\n",
      "Epoch 92/100\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0243 - accuracy: 0.9943\n",
      "Epoch 93/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0287 - accuracy: 0.9938\n",
      "Epoch 94/100\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0192 - accuracy: 0.9960\n",
      "Epoch 95/100\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0192 - accuracy: 0.9956\n",
      "Epoch 96/100\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0384 - accuracy: 0.9914\n",
      "Epoch 97/100\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0254 - accuracy: 0.9943\n",
      "Epoch 98/100\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0247 - accuracy: 0.9945\n",
      "Epoch 99/100\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0286 - accuracy: 0.9931\n",
      "Epoch 100/100\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0197 - accuracy: 0.9953\n",
      "  1/313 [..............................] - ETA: 0s - loss: 0.0164 - accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0006s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1425 - accuracy: 0.9825\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E1C794B318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[7 2 1 0 4]\n",
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# Наш вариант модели, с изменными параметрами \n",
    "###############################\n",
    "import numpy as np\n",
    "import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Flatten the images.\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Dense(512, activation='relu', input_shape=(784,)),\n",
    "  Dense(256, activation='relu'),\n",
    "  Dense(196, activation='relu'),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(48, activation='relu'),\n",
    "  Dense(32, activation='relu'),\n",
    "  Dense(24, activation='relu'),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=100,\n",
    "  batch_size=32,\n",
    ")\n",
    "\n",
    "# Evaluate the model.\n",
    "model.evaluate(\n",
    "  test_images,\n",
    "  to_categorical(test_labels)\n",
    ")\n",
    "\n",
    "# Save the model to disk.\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод 1.1\n",
    "\n",
    "Вот уже рост интереснее, в 2% процента, но при этом это 100 эпох и 10 сеток под капотом. \n",
    "На мой взгляд, преимущество фреймворков огромно и перекрывает большинство их недостатков. Код простой, быстро меняемый и адаптируемый. В нём не сложно разбираться не сильно подготовленному человеку. И всё это делает возможным использовать нейросети в бизнесах и задачах, которые раньше и представить было невозможно, начиная от булочной и заканчивая торговой сетью среднего масштаба.\n",
    "\n",
    "Я не вижу сейчас дальнейшего смысла проводить какие-то эксперименты, так как нужно более глубокое погружение в движок. Чтобы понять, что и как, чтобы сравнивать и смотреть. Потому переключусь на документацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полезные команды Keros \n",
    "\n",
    "Лично для меня, на данном этапе, это задание из разряда \"пойди туда не знаю куда, принеси то не знаю что\". Полезными команды становятся тогда, когда у тебя есть цель определённая. Пока это больше похоже на ситуацию, когда я впервые увидел океан, а меня спрашивают в как я хочу там ловить рыбу. А я пока даже не знаю, какая там ловится рыбы, на что клюет и т.д. :) Но там точно кто-то водится.\n",
    "\n",
    "Но попробуем :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мне кажутся интересными функции касающейся метрик. Мы пока рассматривали только просто функцию ошибок, общую. Применительно к этому дата сету, наша сеть сейчас распознает 10 классов цифр и ориентируются на общую ошибку. Мы можем сделать сеть, которая будет хорошо ловить едины (сложно ошибиться в ловле этой цифры, разве что 7 может помешать), и тогда нам (как мне пока кажется) помогут метрики из разряда: \n",
    "\n",
    ">tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None)\n",
    "\n",
    "Кроме этого, сюда можно ещё отнести и указание весов для бинарных метрик, когда в наших данных есть несбалансированность. в классах.\n",
    "\n",
    "**Кстати, вопрос, а как нейросети относятся к несбалансированным классам? В чистом виде так их реализации в движках?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1875/1875 [==============================] - 2s 889us/step - loss: 0.3500 - accuracy: 0.8959\n",
      "Epoch 2/15\n",
      "1875/1875 [==============================] - 2s 892us/step - loss: 0.1787 - accuracy: 0.9450\n",
      "Epoch 3/15\n",
      "1875/1875 [==============================] - 2s 887us/step - loss: 0.1370 - accuracy: 0.9574\n",
      "Epoch 4/15\n",
      "1875/1875 [==============================] - 1s 766us/step - loss: 0.1170 - accuracy: 0.9630\n",
      "Epoch 5/15\n",
      "1875/1875 [==============================] - 1s 693us/step - loss: 0.0994 - accuracy: 0.9696\n",
      "Epoch 6/15\n",
      "1875/1875 [==============================] - 1s 681us/step - loss: 0.0910 - accuracy: 0.9715\n",
      "Epoch 7/15\n",
      "1875/1875 [==============================] - 1s 681us/step - loss: 0.0832 - accuracy: 0.9729\n",
      "Epoch 8/15\n",
      "1875/1875 [==============================] - 1s 683us/step - loss: 0.0755 - accuracy: 0.9755\n",
      "Epoch 9/15\n",
      "1875/1875 [==============================] - 1s 692us/step - loss: 0.0689 - accuracy: 0.9778\n",
      "Epoch 10/15\n",
      "1875/1875 [==============================] - 1s 708us/step - loss: 0.0668 - accuracy: 0.9782\n",
      "Epoch 11/15\n",
      "1875/1875 [==============================] - 1s 737us/step - loss: 0.0614 - accuracy: 0.9800\n",
      "Epoch 12/15\n",
      "1875/1875 [==============================] - 1s 749us/step - loss: 0.0557 - accuracy: 0.9817\n",
      "Epoch 13/15\n",
      "1875/1875 [==============================] - 1s 718us/step - loss: 0.0556 - accuracy: 0.9819\n",
      "Epoch 14/15\n",
      "1875/1875 [==============================] - 1s 694us/step - loss: 0.0506 - accuracy: 0.9837\n",
      "Epoch 15/15\n",
      "1875/1875 [==============================] - 1s 714us/step - loss: 0.0488 - accuracy: 0.9836\n",
      "  1/313 [..............................] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "313/313 [==============================] - 0s 626us/step - loss: 0.1071 - accuracy: 0.9707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10706145316362381, 0.9707000255584717]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The full neural network code!\n",
    "###############################\n",
    "import numpy as np\n",
    "import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Flatten the images.\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=15,\n",
    "  batch_size=32,\n",
    ")\n",
    "\n",
    "# Evaluate the model.\n",
    "model.evaluate(\n",
    "  test_images,\n",
    "  to_categorical(test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 1, 'epochs': 15, 'steps': 1875}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22e5fb61588>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3iU5Z3/8fd3JucjIQmBJEASziQiaETFqtBWi7Ut7WV7VavW1rZIf1pbrFttd7fb/Xnt1nbdutrVWtvabk+e2ro/tuKRqmyrrQQFJZyJICEgIUBCgJDD3L8/ZhKGkJAJSXjm8HldV66Z53BPvoHkM/fcz/08jznnEBGR+OXzugARERlZCnoRkTinoBcRiXMKehGROKegFxGJc0leF9CXgoICV1ZW5nUZIiIxY/Xq1fucc4V9bYvKoC8rK6OmpsbrMkREYoaZ7ehvm4ZuRETinIJeRCTOKehFROJcVI7Ri0h06ejooL6+nra2Nq9LSXhpaWmUlpaSnJwccRsFvYgMqL6+nuzsbMrKyjAzr8tJWM45mpqaqK+vp7y8POJ2EQ3dmNlCM9tkZlvN7M4+ti8ys7fMbI2Z1ZjZ+8K2bTezt7u3RVyZiESNtrY28vPzFfIeMzPy8/MH/clqwB69mfmBB4DLgHpglZktc86tD9ttBbDMOefMbBbwBDA9bPsC59y+QVUmIlFFIR8dTuf/IZIe/Vxgq3OuzjnXDjwGLArfwTnX6o5f7zgTOOPXPu7oCvDgy1tZubnxTH9rEZGoFknQlwA7w5brQ+tOYGafMLONwNPAjWGbHPC8ma02s8X9fRMzWxwa9qlpbBx8WCf5jIdX1rH87d2Dbisi0a2pqYnZs2cze/Zsxo4dS0lJSc9ye3v7KdvW1NRw6623Dvg95s2bNyy1vvzyy3zkIx8ZltcaLpEcjO3rc8JJPXbn3FPAU2Z2CXAX8MHQpouccw1mNgZ4wcw2OudW9tH+YeBhgOrq6kF/IjAzqopzWdfQPNimIhLl8vPzWbNmDQDf+c53yMrK4vbbb+/Z3tnZSVJS33FWXV1NdXX1gN/j1VdfHZ5io1AkPfp6YHzYcinQ0N/OoRCfZGYFoeWG0ONe4CmCQ0EjorI4h817WmnvDIzUtxCRKPG5z32O2267jQULFnDHHXfw+uuvM2/ePObMmcO8efPYtGkTcGIP+zvf+Q433ngj8+fPp6Kigvvvv7/n9bKysnr2nz9/Pp/85CeZPn061157Ld0j08uXL2f69Om8733v49Zbbx1Uz/3RRx/lrLPOoqqqijvuuAOArq4uPve5z1FVVcVZZ53FvffeC8D999/PzJkzmTVrFldfffWQ/60i6dGvAqaYWTmwC7ga+Ez4DmY2GdgWOhh7DpACNJlZJuBzzh0KPb8c+L9DrroflSW5tHcF2LL3EJXFuSP1bUQS2j//Ty3rG1qG9TVnFufwTx+tHHS7zZs38+KLL+L3+2lpaWHlypUkJSXx4osv8q1vfYvf//73J7XZuHEjL730EocOHWLatGl8+ctfPmlO+ptvvkltbS3FxcVcdNFF/OUvf6G6upqbbrqJlStXUl5ezjXXXBNxnQ0NDdxxxx2sXr2avLw8Lr/8cv77v/+b8ePHs2vXLtatWwfAwYMHAbj77rt55513SE1N7Vk3FAP26J1zncAtwHPABuAJ51ytmS0xsyWh3a4C1pnZGoIzdD4dOjhbBPzZzNYCrwNPO+eeHXLV/agqzgGgdph/CUUkOn3qU5/C7/cD0NzczKc+9SmqqqpYunQptbW1fba58sorSU1NpaCggDFjxvDee++dtM/cuXMpLS3F5/Mxe/Zstm/fzsaNG6moqOiZvz6YoF+1ahXz58+nsLCQpKQkrr32WlauXElFRQV1dXV85Stf4dlnnyUnJ5hhs2bN4tprr+XXv/51v0NSgxHRKzjnlgPLe617KOz594Dv9dGuDjh7iDVGrCw/k8wUP7W7mqF6/MANRGTQTqfnPVIyMzN7nv/jP/4jCxYs4KmnnmL79u3Mnz+/zzapqak9z/1+P52dnRHtc3xi4eD11zYvL4+1a9fy3HPP8cADD/DEE0/wyCOP8PTTT7Ny5UqWLVvGXXfdRW1t7ZACP66udePzGTOLc9SjF0lAzc3NlJQEJwT+4he/GPbXnz59OnV1dWzfvh2Axx9/POK2559/Pq+88gr79u2jq6uLRx99lEsvvZR9+/YRCAS46qqruOuuu3jjjTcIBALs3LmTBQsW8P3vf5+DBw/S2to6pNrj7hIIlcW5PFGzk66Aw+/TCR4iieIb3/gGN9xwAz/4wQ94//vfP+yvn56ezoMPPsjChQspKChg7tz+55WsWLGC0tLSnuUnn3yS7373uyxYsADnHB/+8IdZtGgRa9eu5fOf/zyBQHACyXe/+126urq47rrraG5uxjnH0qVLGTVq1JBqt6F8HBkp1dXV7nRvPPJkzU7+7ndvseLrlzKpMGuYKxNJTBs2bGDGjBlel+G51tZWsrKycM5x8803M2XKFJYuXXrG6+jr/8PMVjvn+pxHGldDN0DPbJt1uzSfXkSG109+8hNmz55NZWUlzc3N3HTTTV6XFJG4G7qZUpRFit/H+oYWFs0+6QReEZHTtnTpUk968EMVdz36ZL+PaWOzdYasyDCLxmHeRHQ6/w9xF/QAVSXBmTf6xRQZHmlpaTQ1NelvymPd16NPS0sbVLu4G7oBmFmcy6Ov72TXwaOU5mV4XY5IzCstLaW+vp7TueCgDK/uO0wNRlwGffgZsgp6kaFLTk4e1B2NJLrE5dDN9LE5+IzgGbIiIgkuLoM+PcXP5DFZOkNWRIQ4DXoIzqfXzBsRkbgO+hzeazlG46FjXpciIuKpOA764BmyterVi0iCi9ugn6lr04uIAHEc9LnpyUwYnaEevYgkvLgNejh+hqyISCKL66CvLM5lR9MRWto6vC5FRMQzcR70wXH64b6RsYhILInzoNe16UVE4jroC7NTKcpJ1Ti9iCS0uA56CPbqNfNGRBJZ3Ad9VXEOW/e2crS9y+tSREQ8EfdBX1mSS8DBxj0avhGRxBRR0JvZQjPbZGZbzezOPrYvMrO3zGyNmdWY2fsibTvSumferNM4vYgkqAGD3sz8wAPAFcBM4Bozm9lrtxXA2c652cCNwE8H0XZElYxKZ1RGMus1Ti8iCSqSHv1cYKtzrs451w48BiwK38E51+qO30wyE3CRth1pZkZlcQ7rdqlHLyKJKZKgLwF2hi3Xh9adwMw+YWYbgacJ9uojbhtqvzg07FMz3PelrCrOZdOeQ3R0BYb1dUVEYkEkQW99rDvpVvDOuaecc9OBjwN3DaZtqP3Dzrlq51x1YWFhBGVFbmZxDu1dAba81zqsrysiEgsiCfp6YHzYcinQ0N/OzrmVwCQzKxhs25FSVaJr04tI4ook6FcBU8ys3MxSgKuBZeE7mNlkM7PQ83OAFKApkrZnQnl+Jhkpfp0hKyIJKWmgHZxznWZ2C/Ac4Acecc7VmtmS0PaHgKuAz5pZB3AU+HTo4GyfbUfoZ+mXz2fMHJejHr2IJKQBgx7AObccWN5r3UNhz78HfC/Stl6oLM7hd6vrCQQcPl9fhw5EROJT3J8Z262yJJfD7V1sbzrsdSkiImdU4gS9zpAVkQSVMEE/ZUw2KX6fxulFJOEkTNCnJPmYOjaLWp0hKyIJJmGCHoJnyNY2NHP8ag0iIvEvoYK+sjiHA0c6aGhu87oUEZEzJrGCvvsMWd1DVkQSSEIF/YyxOfhMM29EJLEkVNCnp/iZVJila9OLSEJJqKAHdG16EUk4CRf0VSW57GlpY1/rMa9LERE5IxIu6GeGzpDVlSxFJFEkXNBXFgdn3qzTzBsRSRAJF/S56cmMH53OevXoRSRBJFzQQ/AM2XWaeSMiCSIxg74klx1NR2hp6/C6FBGREZeQQd99QFbDNyKSCBIy6KuKu28WrqAXkfiXkEFfmJ3KmOxUXfNGRBJCQgY9BMfp1aMXkUSQsEFfWZzD1sZW2jq6vC5FRGREJXDQ59IVcGzcc8jrUkRERlQCB33oZuEapxeROJewQV+al05uerLG6UUk7kUU9Ga20Mw2mdlWM7uzj+3Xmtlboa9XzezssG3bzextM1tjZjXDWfxQmBmVxTnU6gxZEYlzAwa9mfmBB4ArgJnANWY2s9du7wCXOudmAXcBD/favsA5N9s5Vz0MNQ+bqpJcNu45REdXwOtSRERGTCQ9+rnAVudcnXOuHXgMWBS+g3PuVefcgdDiX4HS4S1zZFQW59DeGWDr3lavSxERGTGRBH0JsDNsuT60rj9fAJ4JW3bA82a22swW99fIzBabWY2Z1TQ2NkZQ1tBV6gxZEUkAkQS99bHO9bmj2QKCQX9H2OqLnHPnEBz6udnMLumrrXPuYedctXOuurCwMIKyhq68IJP0ZL9m3ohIXIsk6OuB8WHLpUBD753MbBbwU2CRc66pe71zriH0uBd4iuBQUFTw+4yZxTm6uJmIxLVIgn4VMMXMys0sBbgaWBa+g5lNAP4AXO+c2xy2PtPMsrufA5cD64ar+OHQPfMmEOjzQ4qISMwbMOidc53ALcBzwAbgCedcrZktMbMlod2+DeQDD/aaRlkE/NnM1gKvA087554d9p9iCKqKcznc3sWO/Ue8LkVEZEQkRbKTc245sLzXuofCnn8R+GIf7eqAs3uvjyYzw86QLS/I9LgaEZHhl7BnxnabWpRNst8080ZE4lbCB31Kko+pRdk6Q1ZE4lbCBz0Ex+lrG1pwTgdkRST+KOiBypIc9h9uZ3dzm9eliIgMOwU9x8+Q1YlTIhKPFPTAjHHZ+EyXQhCR+KSgBzJSkqgozNIBWRGJSwr6kKriHPXoRSQuKehDKotz2d3cRlPrMa9LEREZVgr6kMqS4Bmy6tWLSLxR0IdUjgvNvNE4vYjEGQV9SG5GMuNHp6tHLyJxR0EfpnJcLrWaSy8icUZBH6aqJIftTUc41NbhdSkiIsNGQR+m+wxZ3XFKROKJgj6MZt6ISDxS0IcZk51GYXaqZt6ISFxR0PdSpZuFi0icUdD3Ulmcy5a9rbR1dHldiojIsFDQ91JVkkNXwLFpzyGvSxERGRYK+l56rk2vcXoRiRMK+l5K89LJSUvSzBsRiRsK+l7MjMpinSErIvFDQd+HqpIcNuw5REdXwOtSRESGLKKgN7OFZrbJzLaa2Z19bL/WzN4Kfb1qZmdH2jYaVRbn0t4ZYFtjq9eliIgM2YBBb2Z+4AHgCmAmcI2Zzey12zvApc65WcBdwMODaBt1qrrPkN2lcXoRiX2R9OjnAludc3XOuXbgMWBR+A7OuVedcwdCi38FSiNtG43KC7JIT/Zr5o2IxIVIgr4E2Bm2XB9a158vAM8Mtq2ZLTazGjOraWxsjKCskeP3GTPGZWvmjYjEhUiC3vpY5/rc0WwBwaC/Y7BtnXMPO+eqnXPVhYWFEZQ1siqLc1nf0EIg0Ge5IiIxI5KgrwfGhy2XAg29dzKzWcBPgUXOuabBtI1GVSU5tB7rZMf+I16XIiIyJJEE/SpgipmVm1kKcDWwLHwHM5sA/AG43jm3eTBto1X3GbK1GqcXkRg3YNA75zqBW4DngA3AE865WjNbYmZLQrt9G8gHHjSzNWZWc6q2I/BzDLupRdkk+411mnkjIjEuKZKdnHPLgeW91j0U9vyLwBcjbRsLUpJ8TC3KVo9eRGKezow9hcriHGobWnBOB2RFJHYp6E+hqiSX/Yfb2dPS5nUpIiKnTUF/CpXFwTNkNU4vIrFMQX8KM8blYKaZNyIS2xT0p5CRkkRFQaZ69CIS0xT0A6gqyWW9evQiEsMU9AOoLM6hobmN/YfbvS5FROS0KOgHUFUSPEP26bdi4soNIiInUdAP4PzyfC6eUsBdf9zAmp0HvS5HRGTQFPQD8PuM+6+ew5icVL7869Xsaz3mdUkiIoOioI9AXmYKD113LvsPt3Pzb97QvWRFJKYo6CNUVZLL3Vedxd/e2c+/Lt/gdTkiIhGL6KJmEvSJOaW8Vd/Mz/+ynVmluXxiTunAjUREPKYe/SB968MzOL98NN/8w9s6Y1ZEYoKCfpCS/T7+8zPnMCo9hZt+tZoDml8vIlFOQX8aCrNTeej6c9nbcoxbH3uTLt1XVkSimIL+NM0eP4q7Pl7J/27Zx789t8nrckRE+qWgH4JPnzeBz5w/gYde2cbyt3d7XY6ISJ8U9EP0Tx+dyZwJo7j9ybVsfu+Q1+WIiJxEQT9EqUl+HrruXDJSkrjpV6tpPtrhdUkiIidQ0A+Dopw0fnTdOezcf4TbHl9DQAdnRSSKKOiHyXllo/n2R2eyYuNe7luxxetyRER6KOiH0fUXTOSqc0q5b8UWXlz/ntfliIgAEQa9mS00s01mttXM7uxj+3Qze83MjpnZ7b22bTezt81sjZnVDFfh0cjM+JdPVFFVksPSx9dQ19jqdUkiIgMHvZn5gQeAK4CZwDVmNrPXbvuBW4F7+nmZBc652c656qEUGwvSkoMHZ5OTfNz0q9W0Huv0uiQRSXCR9OjnAludc3XOuXbgMWBR+A7Oub3OuVWAppwApXkZ/Oc1c9jW2MrfPbkW53RwVkS8E0nQlwA7w5brQ+si5YDnzWy1mS3ubyczW2xmNWZW09jYOIiXj07zJhfwzStm8My6PfzolW1elyMiCSySoLc+1g2mi3qRc+4cgkM/N5vZJX3t5Jx72DlX7ZyrLiwsHMTLR68vXlzOR88u5p7nNrFyc+y/eYlIbIok6OuB8WHLpUDEd8p2zjWEHvcCTxEcCkoIZsb3rjqLqUXZfOXRN9m5/4jXJYlIAook6FcBU8ys3MxSgKuBZZG8uJllmll293PgcmDd6RYbizJSkvjx9efinGPxr1ZztL3L65JEJMEMGPTOuU7gFuA5YAPwhHOu1syWmNkSADMba2b1wG3AP5hZvZnlAEXAn81sLfA68LRz7tmR+mGi1cT8TO6/Zg4b97TwzT+8pYOzInJGRXQrQefccmB5r3UPhT3fQ3BIp7cW4OyhFBgv5k8bw9cvm8o9z2/mrNJRfOF95V6XJCIJQmfGnkH/Z/5kPlRZxL8u38Br25q8LkdEEoSC/gzy+Yx7PnU2ZfkZ3PLbN2g4eNTrkkQkASjoz7DstGQe/mw1xzoDfOmXNbyz77DXJYlInFPQe2BSYRY/vGYO7zYd4UP3ruTfn9+k2TgiMmIU9B5ZMH0MK75+KVfOGscP/7SVD/7gFZ6v3aMZOSIy7BT0HhqTk8a9n57N44svIDPVz+JfreYL/1XDjiYN54jI8FHQR4HzK/J5+taL+YcrZ/C3uiYuu3cl976wmbYODeeIyNAp6KNEst/HFy+u4E+3z2dh5VjuW7GFy+9dyZ826gYmIjI0CvooU5STxv3XzOG3XzyfZL9x4y9q+NIva3SdHBE5bQr6KDVvcgHPfPUS7rxiOn/Zuo8P/uAVfrhiC8c6NZwjIoOjoI9iKUk+llw6iRdvu5QPziji31/YzIfuXckruuSxiAyCgj4GFI9K54Frz+GXN87FZ8YNj7zOkl+tZpfOrBWRCCjoY8glUwt55msX83cfmsbLm/fywX9/hQdf3kp7Z8Dr0kQkiinoY0xqkp+bF0zmxdsu5ZKpBXz/2U0svG8lf96yz+vSRCRKKehjVGleBj++vpqff/48ugKO6372N27+7RvsbtZwjoicSEEf4xZMG8NzX7uE2y6byovr3+MDoeEcXTtHRLop6ONAWrKfWz8whRdvu5R5k4LDOfPveYnf/u1dOro0fi+S6BT0cWT86Ax+ekM1T9x0IePzMvjWU29z+b0r+eNbDQQCuliaSKJS0MehueWjeXLJhfzshmpS/D5u+e2bfOyBP7Nyc6OujimSgBT0ccrM+MCMIpZ/9WLu/fTZHDzSwWcfeZ3P/ORvvPnuAa/LE5EzyKKxh1ddXe1qamq8LiOuHOvs4rHXd/LDP21hX2s7H6os4vbLpzGlKNvr0kRkGJjZaudcdZ/bFPSJ5fCxTh758zv8eGUdR9o7ueqcUr522VRKRqV7XZqIDIGCXk6y/3A7D760lV/+dQc4uP7Cidy8YDKjM1O8Lk1EToOCXvq16+BR7ntxM79bXU9GShJfuriCL1xcTlZqktelicggnCroIzoYa2YLzWyTmW01szv72D7dzF4zs2Nmdvtg2oq3Skal8/1Pns3zSy/hfZMLuPfFzVz6/Zf4+V/e0SWRReLEgD16M/MDm4HLgHpgFXCNc2592D5jgInAx4EDzrl7Im3bF/XovbNm50G+98xGXqtromRUOrddNpWPzynB7zOvSxORUxhqj34usNU5V+ecawceAxaF7+Cc2+ucWwV0DLatRJfZ40fx2y+dz6++MJe8zGS+/uRarrhvJS+sf09z8EViVCQDsSXAzrDleuD8CF8/4rZmthhYDDBhwoQIX15Ggplx8ZRCLppUwDPr9nDP85v40i9rKMxO5YKKfOZNyufCinwm5mdgpp6+SLSLJOj7+kuOtGsXcVvn3MPAwxAcuonw9WUE+XzGlbPGcXllEX98q4GXNjbyWl0T/7O2AYBxuWlcGAr9CyflU5qX4XHFItKXSIK+HhgftlwKNET4+kNpK1Ei2e/jE3NK+cScUpxzbGs8zGt1Tby2bR8vb2rkD2/sAmD86HTmVRQEw39SPkU5aR5XLiIQWdCvAqaYWTmwC7ga+EyErz+UthKFzIzJY7KYPCaL6y+YSCDg2Lz3EK9ta+LVbU08s243j9cER+sqCjJ7Qv+CinwKslI9rl4kMUU0j97MPgz8B+AHHnHO/YuZLQFwzj1kZmOBGiAHCACtwEznXEtfbQf6fpp1E7u6Ao4Nu1tCwb+PVdsP0HqsE4BpRdk9oX9BxWhGZejkLJHhohOmxDOdXQHe3tUcGuppYtX2/bR1BDCDmeNyuLAin/PKR1M9MY989fhFTpuCXqJGe2eAtfUHe3r8b7x7sOfm5hWFmZw3cTTVZXlUl42mTLN6RCKmoJeo1dbRxbpdzdTsOEDN9v2s2n6A5qPB0zEKslKoDgX/eWWjmVmcQ7JfV9YW6cupgl4XNBFPpSX7qS4bTXXZaLh0EoGAY1tjK6u2h4J/x36erd0DQHqynzkTRlE9MdjjnzNhFNlpyR7/BCLRTz16iXrvtbRRs/0Aq7bvp2bHftY3tBBw4DOYMS6H88pCwz0TRzM2V1M6JTFp6EbiSuuxTt5890BPr//Ndw9ytCN4AbbSvPSe4D+vbDSTC7Pw6To9kgA0dCNxJSs1iYunFHLxlEIAOroCbNjd0hP8/7tlH0+9GTyJKzc9mXMn5vX0+GeV5pKW7PeyfJEzTj16iTvOOXY0HQk7wLufbY2HAUjx+zirNLdnnP/ciXm62YrEBQ3dSMLbf7id1WHB//auZjq6gr/7kwozQ8M9wfn8ulibxCIFvUgvbR1dvFXfzKrt+3veAFragmfwFmSlhnr8mtYpsUNj9CK9pCX7mVs+mrnlowEIBBxbG1uDM3tCM3zCp3XOHj+K88rymDMhj6qSXAqzdRavxA716EX6sae5jZodweAPn9YJUJSTylkluVSV5HJW6GuMrtYpHlKPXuQ0jM1N4yOzivnIrGIgOK1z3a7mnq+3dzWzYuNeuvtKhdknh39RTqrG+8VzCnqRCGWlJoWuvJnfs671WCcbdrfwdv3x8H95096enn9BVkpP8FeFvopz0xT+ckYp6EWGICs1ifPKRnNe2eiedUfaj4f/27taqG1o5n+37KMrlP6jM7vDP4eq4mD4j81N0wFfGTEKepFhlpGSxLkTR3PuxOPhf7S9iw17WsKGfVr48St1dAaOHyPLTPGTk55Mbnpyz2Pvr5z0pF7LwcfUJJ0EJv1T0IucAekpfs6ZkMc5E/J61rV1dLFxzyHWN7Swr/UYzUc7Tvjauf8I64520HK0g8PtXad8/bRkXzD4046/CeRlplBekNlzR7CJozNI0qeGhKSgF/FIWmja5uzxowbct6MrQEuvN4Lm0JvAicudNB/tYHdzG+samvnd6vqe10jx+ygryAgFfzaTx2QxZUwW5QWZuixEnFPQi8SAZL+P/KzUQd+F61BbB9saD7N1bytb9h5i295W1je08Oy6PT0HjH0G40dnMGVMFpPGZDEl9CYweUwWWamKiHig/0WROJadltznp4a2ji7e2XeYLXtb2bq3lW2hN4JXNjf2XBoCYFxuWk/oTx6TxeTCLKYUZev6QDFGQS+SgNKS/cwYl8OMcTknrO/sCvDu/iM9bwDdX4+v2smRsOMEBVmpTBubxdSibKYVZTN1bDZTxmTpRjBRSkEvIj2S/D4qCrOoKMziQ5XH1wcCjobmo8EhoPda2fzeITa/d4jHXt/Zcy8AgJJR6UwtymLq2NAbQFFwGEjHALyloBeRAfl8RmleBqV5GcyfNqZnfSDgqD9wlE2h4N+0J/j45637eoaAfAZl+ZlMLco+4U2grCBT5w6cIQp6ETltPp8xIT+DCfkZXDazqGd9R1eAHU2H2bSnNfgmEHoDeH798YPAyX5jUmFo+GdsNvmhcf/uk4aNnifdz3rOKLbw/cL2733Csd9njMtNo6Igi7wEPq6goBeRYZfs94WmcGZzJeN61rd1dPXMANq0JzgEtHrHAZatbRjxmkZlJFNRkBkamsqkoiD4ODE/I+5POIso6M1sIXAf4Ad+6py7u9d2C23/MHAE+Jxz7o3Qtu3AIaAL6Ozv6moiEv/Skv091/wJ13qsk0NtHT0XiHME7xQGEH6B3ePbj29zPdtcWNvguq6AY9fBI9Q1HmZb42HqGltZubnxhPMLfAaleRlUFGZSHnojmBR6jJeL0g0Y9GbmBx4ALgPqgVVmtsw5tz5styuAKaGv84EfhR67LXDO7Ru2qkUkrmSlJo3YnP1pY7N5//QT1x1q6+CdfYepazxM3b7gG0Bd42H+Vrf/hIPLGSn+nvAPfhrIZFJhFhPyM8hOTYqZN4FI/mXnAludc3UAZvYYsAgID/pFwC9d8C31r2Y2yszGOed2D3vFIiJDlJ2WzKzSUcwqPfH8gkDAsaelLfQm0Br8FLDvMGt2HuCPbzWc8OnCZ8E3qOy0ZIGjkSkAAAYHSURBVLLTgm9UWWnB5eD6pJMeu7cF90siOzWZtGTfiL9hRBL0JcDOsOV6Tuyt97dPCbCb4Cep583MAT92zj3c1zcxs8XAYoAJEyZEVLyIyHDy+YziUekUj0rnoskFJ2xr6+hiR9MR6hpb2XngCC1HO0NDTsFhp9Zjnew/3M67TUdoaeuk9VgHbR2BAb+n32c9bwTFuek8seTCYf+5Ign6vt5qet+W6lT7XOScazCzMcALZrbRObfypJ2DbwAPQ/AOUxHUJSJyxqQl+5k2NjhDKFIdXQEO97wZdPYci+h+g+hZbuvk0LFOUkZoumkkQV8PjA9bLgV6HyLvdx/nXPfjXjN7iuBQ0ElBLyISb5L9PkZlpDAqw9upnZG8fawCpphZuZmlAFcDy3rtswz4rAVdADQ753abWaaZZQOYWSZwObBuGOsXEZEBDNijd851mtktwHMEp1c+4pyrNbMloe0PAcsJTq3cSnB65edDzYuAp0IHGpKA3zrnnh32n0JERPplzkXfcHh1dbWrqanxugwRkZhhZqv7O09JF5oQEYlzCnoRkTinoBcRiXMKehGROKegFxGJc1E568bMGoEdp9m8AIiVC6jFUq0QW/XGUq0QW/XGUq0QW/UOpdaJzrnCvjZEZdAPhZnVxMqlkGOpVoitemOpVoitemOpVoitekeqVg3diIjEOQW9iEici8eg7/MyyFEqlmqF2Ko3lmqF2Ko3lmqF2Kp3RGqNuzF6ERE5UTz26EVEJIyCXkQkzsVN0JvZQjPbZGZbzexOr+s5FTMbb2YvmdkGM6s1s696XdNAzMxvZm+a2R+9rmUgoXsW/87MNob+jYf/3mzDxMyWhn4H1pnZo2aW5nVN4czsETPba2brwtaNNrMXzGxL6DHPyxq79VPrv4V+D94ys6fMbNSpXuNM6qvesG23m5kzs4K+2g5WXAS9mfmBB4ArgJnANWY209uqTqkT+LpzbgZwAXBzlNcL8FVgg9dFROg+4Fnn3HTgbKK0bjMrAW4Fqp1zVQTv93C1t1Wd5BfAwl7r7gRWOOemACtCy9HgF5xc6wtAlXNuFrAZ+OaZLuoUfsHJ9WJm44HLgHeH6xvFRdATvD3hVudcnXOuHXgMWORxTf1yzu12zr0Ren6IYBCVeFtV/8ysFLgS+KnXtQzEzHKAS4CfATjn2p1zB72t6pSSgHQzSwIyOPk2nZ4K3d95f6/Vi4D/Cj3/L+DjZ7SofvRVq3PueedcZ2jxrwRvcxoV+vm3BbgX+AYn35v7tMVL0JcAO8OW64ni4AxnZmXAHOBv3lZySv9B8Bdv4Fvae68CaAR+Hhpq+mnoNpZRxzm3C7iHYM9tN8FbcD7vbVURKXLO7YZgpwUY43E9kboReMbrIk7FzD4G7HLOrR3O142XoLc+1kX9vFEzywJ+D3zNOdfidT19MbOPAHudc6u9riVCScA5wI+cc3OAw0TP0MIJQmPbi4ByoBjINLPrvK0qPpnZ3xMcMv2N17X0x8wygL8Hvj3crx0vQV8PjA9bLiXKPgL3ZmbJBEP+N865P3hdzylcBHzMzLYTHBJ7v5n92tuSTqkeqHfOdX9C+h3B4I9GHwTecc41Ouc6gD8A8zyuKRLvmdk4gNDjXo/rOSUzuwH4CHCti+4ThyYRfNNfG/p7KwXeMLOxQ33heAn6VcAUMys3sxSCB7SWeVxTvyx4t/SfARuccz/wup5Tcc590zlX6pwrI/jv+ifnXNT2Op1ze4CdZjYttOoDwHoPSzqVd4ELzCwj9DvxAaL0wHEvy4AbQs9vAP6fh7WckpktBO4APuacO+J1PafinHvbOTfGOVcW+nurB84J/U4PSVwEfehgyy3AcwT/UJ5wztV6W9UpXQRcT7B3vCb09WGvi4ojXwF+Y2ZvAbOBf/W4nj6FPnX8DngDeJvg32NUna5vZo8CrwHTzKzezL4A3A1cZmZbCM4OudvLGrv1U+t/AtnAC6G/s4c8LTJMP/WOzPeK7k8yIiIyVHHRoxcRkf4p6EVE4pyCXkQkzinoRUTinIJeRCTOKehFROKcgl5EJM79fzF9zTVQ83WhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "#plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот ещё нашел что fit возвращает объект по которому можно быстро построить как менялась ошибка. Визуализация - всегда хорошо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мне интересным аспектом мне показалась работа с временными рядами (детектирование выбросов или не нормально поведения, типа как поиск планет по кривой блеска), так как это пока вообще самое интересное и быстро применимое в каких-то реальных бизнес задачах, которые уже сейчас доступны (это я в целом про временные ряды). Но пока сложно, надо поближе с движком познакомится."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
